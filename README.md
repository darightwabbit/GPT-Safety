# GPT-Safety

Overview

These documents present an independent risk analysis of safety filters deployed on ChatGPT between August-October 2025. 
While these measures were implemented with protective intent, this analysis identifies potential mechanisms through which they could inadvertently expose hundreds of millions of users to psychological risks.
Key Finding: Filters designed to protect 0.15% of users (~1.2M people) may expose 100% of the user base (800M+ weekly users) to repetitive concepts related to suicide and mental distress—potentially creating the opposite effect intended.

Core Argument

OpenAI deployed strengthened safety filters following high-profile incidents, consulting 170+ mental health experts. However, this analysis raises urgent questions:

Risk Displacement: Protective measures for a tiny fraction may create broader exposure through frequent false positives
Neurodivergent Impact: ~100-150M autistic, ADHD, OCD, and anxious users may lose judgment-free communication space
Werther Effect at Scale: Daily exposure to suicide-related alerts (vs. punctual media exposure) may amplify contagion risk
Opacity & Instability: Unpredictable, changing trigger rules create chronic stress environment

What These Documents Are

A call for transparent evaluation and independent audit
A risk-based analysis grounded in suicide prevention research
An articulation of plausible mechanisms requiring investigation
A framework for evidence-based safety practices

What These Documents Are Not

An accusation or claim of proven harm
A demand to remove all safety measures
Anti-AI or anti-OpenAI advocacy
Medical advice or crisis intervention guidance

Key Concepts Explored

1. Priming Effect
Repeated exposure to suicidal concepts increases their cognitive availability—even in initially unaffected individuals.

3. Werther Effect
Media reporting of suicide can trigger imitative behavior. Unlike traditional media (punctual exposure), ChatGPT filters may create near-daily exposure for heavy users.

5. Risk Displacement
Protecting 1.2M potentially vulnerable users while exposing 800M to triggering content represents a potential net increase in risk.

7. Neurodivergent Exclusion
Autistic, ADHD, OCD, and anxious populations (12-18% of users) rely on ChatGPT as a judgment-free space. Intrusive filters may eliminate this safe haven.

9. False Positive Cascade
Common expressions ("I could kill for a coffee," "this project is killing me," "I'm dead tired") may trigger inappropriate crisis responses.

Scale of Concern

PopulationWeekly Users (est.)Potential ImpactTotal user base800MUniversal exposure to crisis messagingTarget vulnerable users~1.2M (0.15%)Intended protectionNeurodivergent users100-150M (12-18%)Loss of safe communication spaceHypothetical risk (illustrative)400K-600KNew ideation from overexposure (0.05-0.075%)

Note: Figures are illustrative scenarios to demonstrate potential scale, not predictions.

Author Background

20 years of professional experience in prevention/security and risk assessment. This analysis applies established prevention frameworks to evaluate emerging AI safety practices.

Contact: darightwabbit@gmail.com
